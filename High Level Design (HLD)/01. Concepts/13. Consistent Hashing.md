# ğŸ§­ **Consistent Hashing**

Consistent Hashing is a **technique used in distributed systems** to assign keys (like user IDs or cache entries) to nodes (servers or partitions) in a way that minimizes **key remapping** when nodes are added or removed. It enables **scalable, fault-tolerant, and efficient** data partitioning.

---

## ğŸ“– What It Is: **Theoretical Overview**

Traditional hash-based distribution methods (like `key % N`) break down when the number of nodes changes. Adding or removing a node causes **almost every key** to be remapped, leading to **huge data migrations** and system churn.

Consistent hashing solves this by:

- **Mapping both nodes and keys into a hash ring (a circle)**
- Assigning a key to the **first node clockwise** from its hash
- Ensuring that when a node is added or removed, **only a small set of keys** need to be redistributed

This is especially useful for systems with **frequent scaling, dynamic node membership, or volatile traffic**.

---

## ğŸ§® Core Concepts

### 1. **Hash Ring (Circular Hash Space)**

- The output of a hash function (like MD5 or SHA-256) is treated as a **number space**, often from `0` to `2Â³Â² - 1`.
- This space is visualized as a **ring** where the ends wrap around.

### 2. **Node Placement**

- Each node is hashed to a position on the ring using a deterministic hash function.
- Nodes are not uniformly distributed unless **virtual nodes** are used.

### 3. **Key Placement**

- A key (e.g., user ID, object key) is hashed to a point on the ring.
- It is assigned to the **first node** encountered when moving clockwise.

### 4. **Node Addition**

- When a node joins, only keys that fall between the **previous node and the new node** are reassigned.
- This leads to **O(K/N)** key movement, where `K = total keys`, `N = total nodes`.

### 5. **Node Removal**

- Similar logic: only keys assigned to the leaving node need to be remapped.

---

## ğŸ“ˆ Why It Matters

| Scenario                 | Traditional Hashing     | Consistent Hashing         |
| ------------------------ | ----------------------- | -------------------------- |
| Adding a node            | Remap **all keys**      | Remap **\~K/N keys**       |
| Removing a node          | Remap **all keys**      | Remap **\~K/N keys**       |
| Balancing load           | Uneven, needs rebalance | Natural with virtual nodes |
| Use in distributed cache | Not scalable            | Works at massive scale     |

---

## ğŸ§± Practical Need in Distributed Systems

Consistent hashing is a **backbone technique** in:

- Distributed key-value stores (Redis Cluster, Cassandra, DynamoDB)
- Content delivery and routing systems (CDNs, reverse proxies)
- Sharded messaging systems (Kafka, WhatsApp backend)
- Load balancing with session stickiness

---

## ğŸ”„ Example Use Case: Redis Cluster

Redis Cluster uses **16384 hash slots**. Each key is hashed to one of these slots. Slots are assigned to nodes based on consistent hashing. When a node is added:

- Only some slots (and their keys) move
- Clients are aware of which slots are hosted by which node

---

## âš–ï¸ Trade-offs & Challenges

### âœ… Pros

- **Scalable**: Add/remove nodes with minimal rebalancing
- **Fault tolerant**: Node failures donâ€™t impact the whole system
- **Predictable**: Keys map consistently without central coordination

### âš ï¸ Cons

- **Skew**: Without virtual nodes, load distribution may be uneven
- **Complexity**: More logic required in clients or load balancers
- **Split-brain**: If multiple clients use slightly different rings (due to inconsistency), issues may arise

---

## ğŸ” Virtual Nodes (vnodes)

### ğŸ“– What They Are

Each physical node is assigned **multiple hash positions** on the ring (virtual nodes). This ensures:

- **Better key distribution**
- **Smoother load balancing**
- **Easier rebalancing**

### ğŸ› ï¸ Real-world Parallel

Amazon Dynamo uses 100-200 virtual nodes per physical machine. When a machine fails, its virtual nodes can be redistributed evenly.

---

## ğŸ”¬ Hash Function Quality

The hash function must be:

- **Deterministic**: Same input â†’ same output
- **Uniform**: Spread output evenly over the ring
- **Fast**: Low computational cost

Popular choices:

- `MD5` (legacy, but uniform)
- `SHA-1` (used in Git, Cassandra)
- `xxHash`, `MurmurHash` (fast and modern)

---

## ğŸ§ª Code Example (Python)

Hereâ€™s a minimal working implementation:

```python
import hashlib
import bisect

def hash_function(key):
    return int(hashlib.md5(key.encode()).hexdigest(), 16)

class ConsistentHash:
    def __init__(self, replicas=3):
        self.replicas = replicas
        self.ring = dict()
        self.sorted_keys = []

    def add_node(self, node):
        for i in range(self.replicas):
            v_node = f"{node}-{i}"
            key = hash_function(v_node)
            self.ring[key] = node
            bisect.insort(self.sorted_keys, key)

    def remove_node(self, node):
        for i in range(self.replicas):
            v_node = f"{node}-{i}"
            key = hash_function(v_node)
            self.sorted_keys.remove(key)
            del self.ring[key]

    def get_node(self, key):
        h = hash_function(key)
        idx = bisect.bisect(self.sorted_keys, h) % len(self.sorted_keys)
        return self.ring[self.sorted_keys[idx]]

# ğŸ§ª Usage
ch = ConsistentHash()
ch.add_node("cache1")
ch.add_node("cache2")
ch.add_node("cache3")

print(ch.get_node("user-1001"))
print(ch.get_node("user-9009"))
```

---

## ğŸ—ï¸ System Design Integration

### ğŸ“¦ CDN/Cache Layer

```text
hash(user_id) â†’ edge_cache_server
```

- Used to route user content requests to consistent CDN edge servers
- Ensures repeat users hit the **same cache node**

### ğŸ“¬ Messaging Queues (Kafka, WhatsApp)

```text
partition_id = hash(phone_number) % N
```

- Each message lands in a queue partition
- Guarantees message ordering per user

---

## ğŸŒ Real-World Systems

| System        | Uses Consistent Hashing for              |
| ------------- | ---------------------------------------- |
| Cassandra     | Data partitioning and replication        |
| Amazon Dynamo | Ring-based storage + quorum reads/writes |
| Riak          | Key-value storage                        |
| Envoy Proxy   | Load balancing with sticky routing       |
| WhatsApp      | Message processing queue routing         |
| Akamai        | Edge request distribution                |

---

## ğŸ§  Summary Table

| Concept           | Description                   | Role in System                    |
| ----------------- | ----------------------------- | --------------------------------- |
| Hash Ring         | 0 â†’ 2Â³Â² circular space        | Mapping of keys and nodes         |
| Virtual Nodes     | Multiple hash points per node | Balances uneven loads             |
| Minimal Remap     | Only K/N keys shift on change | Efficient scaling                 |
| Clockwise Routing | Next node clockwise from key  | Predictable request handling      |
| Used In           | CDNs, caches, DBs, proxies    | Cassandra, Redis, Kafka, WhatsApp |

---

## ğŸ”š Final Takeaways

- **Consistent Hashing** is foundational to modern **high-scale distributed systems**.
- It elegantly solves the **scaling problem of data movement**.
- While it requires more complexity than simple hashing, its **benefits grow exponentially** with system size.
- Used effectively with **virtual nodes, good hash functions, and routing awareness**, it becomes a **powerful building block** in system design.

---
