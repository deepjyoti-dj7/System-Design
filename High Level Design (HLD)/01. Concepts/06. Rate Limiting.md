# üö¶ **Rate Limiting**

Rate Limiting is the practice of **controlling how many requests a client or user can make** to a system in a given period of time. It protects services from **overuse, abuse, and traffic spikes**, ensuring **fair usage**, **stability**, and **security**.

---

## üìñ What It Is: **Theoretical Overview**

At its core, rate limiting is about defining a **threshold (limit)** for operations per unit time. This could be:

- Requests per second (RPS)
- API calls per minute
- Logins per hour
- Messages per day

The system **tracks the frequency** of client activity and **enforces rules** to reject or delay excess requests.

---

## üß† Why Rate Limit?

| Goal                | Purpose                                        |
| ------------------- | ---------------------------------------------- |
| **Protect backend** | Prevent overload on DBs, caches, compute       |
| **Prevent abuse**   | Deter spamming, scraping, brute force attacks  |
| **Ensure fairness** | No single user can monopolize shared resources |
| **Manage cost**     | Limit expensive operations in SaaS products    |
| **Improve QoS**     | Maintain availability and speed for real users |

---

## ‚öôÔ∏è Common Rate Limiting Algorithms

### 1. ü™£ **Token Bucket**

#### üìñ Theory

- Tokens are added to a bucket at a fixed rate (e.g., 1 token/sec)
- Each request **consumes one token**
- If the bucket is empty, the request is denied or delayed

#### ‚úÖ Features

- **Burst-friendly**: Allows sudden spikes if tokens are saved up
- **Smooth refill**: Prevents overload with continuous refill

#### üß™ Python Example:

```python
import time

class TokenBucket:
    def __init__(self, rate, capacity):
        self.rate = rate            # tokens per second
        self.capacity = capacity
        self.tokens = capacity
        self.last_checked = time.time()

    def allow(self):
        now = time.time()
        elapsed = now - self.last_checked
        self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
        self.last_checked = now
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
```

---

### 2. üìè **Leaky Bucket**

#### üìñ Theory

- A queue (bucket) holds incoming requests
- Requests **leave the bucket at a fixed rate**
- If the bucket is full, incoming requests are dropped

#### ‚úÖ Features

- **Smooth request flow**
- **Deterministic**: Enforces a strict steady rate

---

### 3. üß± **Fixed Window Counter**

#### üìñ Theory

- Count requests in a **fixed time window** (e.g., minute/hour)
- Reset the counter at the end of each window

#### ‚ö†Ô∏è Problem:

- **Edge case burst**: Users can send max requests at window end + start

#### üõ† Example (Redis):

```python
# Pseudocode using Redis
key = f"rate:user:{user_id}:{current_minute}"
if redis.incr(key) > limit:
    reject()
else:
    redis.expire(key, 60)
    allow()
```

---

### 4. ü™ü **Sliding Window Log**

#### üìñ Theory

- Track **timestamps of each request** in a sliding log
- On new request, remove old timestamps and check how many remain

#### ‚úÖ Features

- More accurate than fixed window
- Harder to scale without distributed store

---

### 5. ü™ü **Sliding Window Counter**

#### üìñ Hybrid approach

- Mix of fixed windows and interpolation
- E.g., track counts in 2 adjacent windows and **interpolate** based on current time

---

## üß± Infrastructure-Level Rate Limiting

| Layer             | Example Tools                    | Notes                                       |
| ----------------- | -------------------------------- | ------------------------------------------- |
| **API Gateway**   | Kong, AWS API Gateway, Apigee    | Easy to configure per API key, path, IP     |
| **Nginx / Envoy** | Nginx `limit_req`, Envoy Filters | Network-layer defense for DDoS and crawlers |
| **Redis**         | Use as fast counter store        | TTL + Atomicity (INCR, EXPIRE)              |
| **Middleware**    | Express.js / Django / Flask      | App-layer control per route                 |

---

## üîê Use Cases in Production

| System         | How Rate Limiting is Used                                      |
| -------------- | -------------------------------------------------------------- |
| **GitHub API** | 5,000 API calls per hour per user (OAuth token)                |
| **Slack**      | 1 message/sec per webhook, burstable                           |
| **AWS**        | EC2 instance creation limits to prevent abuse                  |
| **WhatsApp**   | Limits on message sending rate to avoid spam / misuse          |
| **OpenAI API** | Tier-based request quotas (e.g., 60 RPM for free, 10K RPM pro) |

---

## ‚öñÔ∏è Trade-offs and Design Considerations

| Concern             | Design Strategy                                        |
| ------------------- | ------------------------------------------------------ |
| **Fairness**        | Per-user / per-IP / per-token limit buckets            |
| **Cost Control**    | Tiered limits based on pricing plans                   |
| **Low Latency**     | Use in-memory counters (Redis, Memcached)              |
| **Consistency**     | Ensure atomic updates with transactions or Lua scripts |
| **Global vs Local** | Centralized vs node-local limiters (syncing needed)    |

---

## üß™ Distributed Rate Limiting with Redis + Lua (Atomic)

```lua
-- rate_limit.lua
local key = KEYS[1]
local limit = tonumber(ARGV[1])
local current = tonumber(redis.call('INCR', key))

if current == 1 then
  redis.call('EXPIRE', key, 60)
end

if current > limit then
  return 0
else
  return 1
end
```

```python
# Python usage
allowed = redis.eval(rate_limit_lua, 1, f"user:{user_id}", 100)
if allowed == 1:
    process_request()
else:
    reject_request()
```

---

## üîç Strategies by Use Case

| Use Case                   | Recommended Strategy         |
| -------------------------- | ---------------------------- |
| **Login attempts**         | Fixed window (fast + simple) |
| **API gateway throttling** | Sliding window counter       |
| **Messaging/chat**         | Token bucket (allows bursts) |
| **Payment actions**        | Leaky bucket (strict pacing) |
| **Scraper blocking**       | Nginx IP-based fixed window  |

---

## üìä Real-World Design: WhatsApp Message Send Limit

### Goal:

Prevent abuse (bulk messaging, spam bots)

### Approach:

- Token bucket per phone number
- Bucket size = 20, refill = 1 per 3 seconds
- If bucket empty, drop or delay message

### Distributed Enforcement:

- Stored in Redis
- Key: `rate:user:<phone>`
- TTL auto-expiry after inactivity

---

## üß† Summary Table

| Algorithm       | Behavior                   | Best For                  |
| --------------- | -------------------------- | ------------------------- |
| Token Bucket    | Allows bursts + refill     | APIs, UI events           |
| Leaky Bucket    | Strict steady rate         | Streaming, payments       |
| Fixed Window    | Simple, less precise       | Basic API limits          |
| Sliding Log     | Precise, heavy memory      | Login attempts, audits    |
| Sliding Counter | Balanced, accurate + light | SaaS quotas, tiered plans |

---

## üîö Final Takeaways

- **Rate limiting is essential** for both **system protection** and **user fairness**
- Choose your algorithm based on **accuracy vs performance vs memory**
- **Token buckets and Redis** are your friends at scale
- Consider both **global (shared state)** and **local (per-node)** strategies in distributed setups
- Always return proper HTTP status (`429 Too Many Requests`) and headers (e.g., `X-RateLimit-Remaining`)

---
