# üß† **Distributed Caching**

## üìñ What is Distributed Caching?

**Distributed Caching** is a technique where cached data is **stored across multiple nodes in a network**, forming a unified caching layer accessible to many applications or services. It helps avoid repeated, expensive computation or database lookups by storing results closer to where they‚Äôre needed ‚Äî **at scale and with resilience**.

Unlike local or in-process caching (which is bound to a single instance of an application), distributed caching works **across server boundaries**, ensuring all nodes can access and modify a **shared** cache.

> ‚ö° **Purpose**: To reduce latency, decrease load on backend systems, and ensure consistency of frequently used data in **horizontally scaled architectures**.

---

## üß± Why is it Needed?

### 1. **Horizontal Scaling + Stateless Services**

Modern architectures (microservices, containers, Kubernetes) scale horizontally, meaning:

- Multiple app instances are running.
- These instances do **not** share memory.

‚Üí You need a **centralized cache** (but scalable) to serve all instances.

### 2. **Database Bottleneck**

Databases are often the slowest component. Without a cache, a high-traffic service could overwhelm the DB with repeated queries.

‚Üí A distributed cache can handle **millions of reads per second**, offloading the DB.

### 3. **High Availability**

If your cache is distributed with replication and failover, it avoids SPOFs (Single Point of Failures) and can support **fault-tolerant** systems.

---

## ‚öôÔ∏è Key Characteristics

| Property           | Description                                                                        |
| ------------------ | ---------------------------------------------------------------------------------- |
| **Network-based**  | The cache is accessed via TCP/HTTP over the network, not memory-local.             |
| **Shared**         | Multiple clients share the same cache pool.                                        |
| **Distributed**    | Cache data is spread across multiple nodes.                                        |
| **Scalable**       | Nodes can be added/removed without changing client logic (via consistent hashing). |
| **Fault-tolerant** | Replication and redundancy allow for failover and persistence.                     |
| **Fast**           | In-memory (RAM) based storage allows <1ms access time for common lookups.          |

---

## üß† How It Works

### üîπ 1. **Data Distribution (Sharding)**

A distributed cache splits data across nodes (servers) so that each handles a portion of the workload. There are typically two approaches:

- **Modulo-based hashing** (e.g., `hash(key) % N`)

  - Simple but problematic when nodes are added/removed (reshuffling required).

- **Consistent Hashing**

  - Each key is mapped to a position on a virtual ring.
  - Nodes are placed on the ring too.
  - A key is stored in the next clockwise node.
  - Adding/removing a node only affects neighboring keys.

> This ensures **minimal rebalancing** and **predictable routing**, essential for distributed caching.

---

### üîπ 2. **Replication**

For fault tolerance, data is often replicated:

- **Master‚Äìreplica model**: One node handles writes, replicas handle reads.
- **Multi-master replication**: All nodes can read/write (but consistency is harder).

This allows:

- **Failover** if one node crashes.
- **Read scaling** via replicas.

---

### üîπ 3. **Eviction & Expiry Policies**

Since memory is limited, caches must **evict stale data**:

- **TTL (Time-To-Live)**: Data expires after a fixed time.
- **LRU (Least Recently Used)**: Remove least recently accessed items.
- **LFU (Least Frequently Used)**: Remove items accessed the least.

These strategies balance **freshness** and **availability** of data.

---

### üîπ 4. **Cache Consistency**

Unlike databases, caches may return stale data. Strategies include:

- **Write-through cache**: Data is written to cache and DB at the same time.
- **Write-around**: Data is only written to DB, not cache ‚Äî cache is updated only on read.
- **Write-behind**: Data is written to cache first, DB is updated asynchronously.
- **Cache-aside**: Application checks cache first, then DB, and updates cache if DB is hit.

Each strategy has trade-offs in consistency, latency, and complexity.

---

## üß™ Real-World Use Cases

| System      | What Is Cached                            | Benefit                         |
| ----------- | ----------------------------------------- | ------------------------------- |
| **Twitter** | User profiles, tweet timelines            | Sub-millisecond profile lookup  |
| **Netflix** | User session info, personalization models | Avoids DB overload              |
| **Uber**    | Geolocation, fare estimates               | Faster response times           |
| **GitHub**  | Repo metadata, user permissions           | Faster access to static content |
| **Airbnb**  | Property listings, availability calendars | Avoid frequent DB reads         |

---

## üõ† Common Distributed Caching Tools

| Tool          | Highlights                                                 |
| ------------- | ---------------------------------------------------------- |
| **Redis**     | Key-value store with rich data types, persistence, pub/sub |
| **Memcached** | Simple in-memory key-value store, extremely fast           |
| **Hazelcast** | Java-native in-memory data grid                            |
| **Couchbase** | Document-oriented DB with strong caching layer             |
| **Aerospike** | High throughput, low-latency, flash-based storage          |

---

## üîß Sample Architecture

```
                 +-------------------+
                 |  Microservice A   |
                 +-------------------+
                          |
                 +-------------------+
                 |  Microservice B   |
                 +-------------------+
                          |
           +-----------------------------------+
           |      Distributed Cache Layer      |
           |    (e.g., Redis Cluster, Memcached)|
           +-----------------------------------+
                          |
                    +------------+
                    |   Database |
                    +------------+
```

- Cache layer shared across services.
- Stateless app design ‚Äî all state lives in cache/DB.
- Cache nodes are distributed and replicated.

---

## ‚úÖ Advantages

- **Low latency**: Data is retrieved from RAM, not disk.
- **Scalability**: Easily supports growth in users/traffic.
- **Reduced DB load**: Frees database from repetitive queries.
- **Global availability**: Can replicate across regions.
- **High throughput**: Handles hundreds of thousands of operations per second.

---

## ‚ö†Ô∏è Challenges

| Challenge                  | Description                                                                                             |
| -------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Data staleness**         | Cached data may be outdated unless TTL/refresh strategies are used.                                     |
| **Partition tolerance**    | Network failures may cause part of the cache to be unavailable.                                         |
| **Consistency trade-offs** | Writes may not propagate instantly to all nodes (eventual consistency).                                 |
| **Operational complexity** | Cluster management, failover, rebalancing require planning.                                             |
| **Cache stampede**         | Sudden expiration of popular data may overload DB (mitigate with locking, stale-while-revalidate, etc). |

---

## üìã Summary Table

| Concept            | Explanation                                       |
| ------------------ | ------------------------------------------------- |
| Sharding           | Data split across nodes using consistent hashing  |
| Replication        | Increases availability & durability               |
| Eviction policies  | TTL, LRU, LFU used to manage memory               |
| Fault tolerance    | Achieved via replicas and automatic failover      |
| Cache strategies   | Cache-aside, write-through, write-behind          |
| Tooling            | Redis, Memcached, Hazelcast, Aerospike            |
| Monitoring metrics | Hit ratio, latency, memory usage, failover status |

---

## üîö Final Takeaways

- **Distributed caching** is a backbone of modern, high-performance systems.
- It complements stateless microservices, global scalability, and user responsiveness.
- Real-world systems rely on it to **decouple load from persistence layers**, handle traffic spikes, and improve latency.
- **Understanding eviction, consistency, and partitioning** is key to using it effectively.

---
