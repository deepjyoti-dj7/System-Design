# ğŸ§  **Caching Strategies**

Caching is not just about storing dataâ€”**how you read from and write to the cache** (i.e., your strategy) determines system performance, data freshness, and reliability.

Caching strategies define the **interaction pattern between your cache, the application, and the data store** (usually a database or backend API).

---

## ğŸ—‚ï¸ Common Caching Strategies

| Strategy                 | Reads from  | Writes to           | Common Use Case       |
| ------------------------ | ----------- | ------------------- | --------------------- |
| Cache-Aside              | App         | App â†’ Cache & DB    | Most widely used      |
| Read-Through             | App â†’ Cache | Cache â†’ DB fallback | Transparent reads     |
| Write-Through            | App         | Cache + DB (sync)   | Strong consistency    |
| Write-Behind (Backplane) | App         | Cache (async DB)    | High write throughput |
| Refresh-Ahead            | App         | Cache + Preload     | Prevents stale cache  |

---

## 1. ğŸ§¾ **Cache-Aside (Lazy Loading)**

### ğŸ“– What It Is:

The **application code** is responsible for checking the cache and loading from the DB on a miss. Also known as **lazy caching**.

### ğŸ›  How It Works:

```
1. Check cache
2. If miss â†’ fetch from DB
3. Save in cache
4. Return result
```

### âœ… Pros:

- Simple, app-controlled
- Avoids stale data (if TTL is short)

### âš ï¸ Cons:

- First access is **slow**
- Cache **miss stampedes** possible

### ğŸ”§ Example (Python + Redis):

```python
def get_user(user_id):
    if (val := redis.get(f"user:{user_id}")):
        return json.loads(val)

    user = db.fetch_user(user_id)
    redis.setex(f"user:{user_id}", 300, json.dumps(user))
    return user
```

---

## 2. ğŸ“– **Read-Through Cache**

### ğŸ“– What It Is:

The **cache itself handles loading from DB** on a miss. The app talks only to the cache.

### ğŸ›  How It Works:

```
App â†’ Cache â†’ (miss) â†’ Cache loads from DB â†’ Return
```

### âœ… Pros:

- Clean app logic
- Cache always manages itself

### âš ï¸ Cons:

- Needs custom cache layer (or lib support)
- Harder to debug

### ğŸ“¦ Example Use Case:

Used in systems like **Ehcache, Spring Cache**, or **Amazon DAX (for DynamoDB)**.

---

## 3. ğŸ“ **Write-Through Cache**

### ğŸ“– What It Is:

All writes go **through the cache**, which then writes to the DB synchronously.

### ğŸ›  How It Works:

```
App â†’ Write to Cache â†’ Cache writes to DB
```

### âœ… Pros:

- Cache and DB always in sync
- Prevents stale reads

### âš ï¸ Cons:

- **Write latency increases** due to sync DB operation
- Writes fail if DB is down

### ğŸ”§ Example (Pseudo):

```python
def write_user(user_id, data):
    db.update_user(user_id, data)
    cache.set(f"user:{user_id}", json.dumps(data))
```

---

## 4. ğŸ•“ **Write-Behind / Write-Back Cache**

### ğŸ“– What It Is:

App writes to **cache only**, and the cache updates the DB asynchronously (in the background).

### ğŸ›  How It Works:

```
App â†’ Cache â†’ (later) â†’ Cache writes to DB
```

### âœ… Pros:

- Super fast writes
- Batches writes for efficiency

### âš ï¸ Cons:

- Risk of **data loss** if cache crashes before flushing
- Complex failure handling

### ğŸ“¦ Example Use Case:

Used in **high-throughput systems** where DB is a bottleneck (e.g., logging, analytics, counters).

---

## 5. ğŸ” **Refresh-Ahead / Preemptive Refresh**

### ğŸ“– What It Is:

Before cached data expires, the cache **refreshes it in the background**, avoiding sudden spikes on TTL expiry.

### ğŸ›  How It Works:

```
TTL set at 5 min â†’ Refresh triggered at 4.5 min
```

### âœ… Pros:

- Avoids **cache stampedes**
- Guarantees fresh data

### âš ï¸ Cons:

- Needs active monitoring
- May refresh unused entries

### ğŸ“¦ Example:

Used by **Netflix and Google**, especially for **personalized content**, models, and feed generation.

---

## ğŸš¨ Special Topic: Cache Stampede

### ğŸ“– What It Is:

When **many requests simultaneously** hit an expired key, all of them fall back to DB at once.

### ğŸ›  Prevention Techniques:

- Use **Mutex/Locks** (e.g., Redis SETNX) to let only one recompute
- Implement **refresh-ahead**
- Add **randomized TTL** (`ttl + random(0,10)`)

---

## ğŸ”€ Comparison Table

| Strategy      | Read Speed | Write Speed | Freshness | Complexity | Common In          |
| ------------- | ---------- | ----------- | --------- | ---------- | ------------------ |
| Cache-Aside   | Fast       | Normal      | Moderate  | Low        | Web APIs           |
| Read-Through  | Fast       | N/A         | Moderate  | Medium     | Frameworks         |
| Write-Through | Fast       | Slower      | High      | Medium     | Critical systems   |
| Write-Behind  | Fast       | Fast        | Low       | High       | Logs, metrics      |
| Refresh-Ahead | Fast       | Normal      | High      | High       | Personalized feeds |

---

## ğŸ“š Real-World Use Cases

| Company      | Strategy Used            | Use Case                      |
| ------------ | ------------------------ | ----------------------------- |
| **Twitter**  | Cache-aside              | Timeline, trends              |
| **Facebook** | Write-through + Memcache | User profile, friend graph    |
| **Amazon**   | Read-through + TTL       | Product pages, pricing        |
| **Netflix**  | Refresh-ahead + LRU      | Personalized content, models  |
| **Stripe**   | Write-behind (buffered)  | Event logs, idempotency store |

---

## ğŸ§ª Sample Redis TTL Strategy (Node.js)

```js
const redis = require("redis");
const client = redis.createClient();

async function getProduct(id) {
  const cacheKey = `product:${id}`;
  const cached = await client.get(cacheKey);

  if (cached) return JSON.parse(cached);

  const data = await db.getProduct(id);
  await client.setEx(cacheKey, 600, JSON.stringify(data)); // TTL = 10 mins
  return data;
}
```

---

## ğŸ“‘ Final Takeaways

- **Caching strategy = cache access pattern + data lifecycle logic**.
- Use **Cache-aside** for flexibility, **Write-through** for safety, and **Write-behind** for performance.
- Always plan for **expiration, eviction**, and **fallback** behavior.
- Use monitoring (hit rate, TTL, latency) to tune strategies in production.
- Combine strategies! e.g., Cache-aside for reads, Write-behind for writes.

---

## ğŸ§  Summary Table

| Strategy      | Description                     | Best Use Case                    |
| ------------- | ------------------------------- | -------------------------------- |
| Cache-Aside   | App loads from DB on cache miss | General-purpose                  |
| Read-Through  | Cache fetches DB automatically  | Seamless read handling           |
| Write-Through | Writes to cache and DB together | Strong consistency systems       |
| Write-Behind  | Async DB writes from cache      | High-write, eventual consistency |
| Refresh-Ahead | Preloads cache before expiry    | Low-latency, always-fresh data   |

---
