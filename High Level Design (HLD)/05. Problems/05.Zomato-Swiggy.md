# üéØ Zomato/Swiggy System Design ‚Äî High-Level Design (HLD) with Data Estimations

Below is a high-level architecture and quantitative sizing for a food-delivery & restaurant-discovery platform similar to **Zomato/Swiggy**.

> **Assumptions (baseline)**

- Active users: **100M** (monthly active users)
- Peak concurrent users: **2%** of active ‚Üí **2.0M** concurrent
- Restaurants listed: **2M**
- Avg photos per restaurant: **20**
- Avg photo size: **0.5 MB**
- Avg orders per active user per month: **6** ‚Üí **0.2 orders/user/day**
- Delivery partners (drivers): **4M** (registered, with subset active)
- Search queries per user/day: **5**
- Avg order payload/events: small JSON ‚âà 2 KB

---

# üìå Functional Requirements

- User Authentication & Authorization (mobile + web)
- Browse/search restaurants, dishes, cuisines, menus
- Place & track orders in real time (order lifecycle)
- Restaurant onboarding / menu management
- Delivery partner app (accept, navigate, update status)
- Payments integration (cards, wallets, UPI)
- Ratings, reviews, photos
- Personalized recommendations & offers
- Promotions, coupon management, dynamic pricing
- Support for multi-restaurant/ghost kitchens
- Real-time notifications (push, SMS, email)

---

# ‚öôÔ∏è Non-Functional Requirements

- High availability (24√ó7)
- Low latency for search / browse (<200ms anywhere)
- Real-time order tracking (seconds)
- Scalability to 100M+ users and tens of millions of daily orders
- Fault tolerance and durability (orders are critical)
- Data privacy and PCI DSS compliance for payments

---

# üß© High-level Components

- **Client Layer**: iOS, Android, Web, Restaurant Portal, Delivery Partner App
- **API Gateway**: Auth, rate limiting, routing, API composition
- **Microservices**:

  - User Service (profiles, wallets)
  - Restaurant Service (menu, hours, status)
  - Search Service (Elasticsearch)
  - Order Service (transactional order lifecycle)
  - Delivery Service (assignment, tracking)
  - Payment Service (integration with PSPs)
  - Recommendation Service (personalized lists)
  - Review & Rating Service
  - Promotion/Coupon Service
  - Notification Service (push/SMS/email)
  - Analytics & BI

- **Event Streaming**: Kafka (order events, location updates)
- **Datastores**: RDBMS for orders, NoSQL for metadata, Object store for images
- **Third-party**: Maps & routing (Google Maps/Mapbox), Payment Gateways, SMS/Push providers

---

# üìä Estimated Data Metrics & Calculations

### 1) Orders & Throughput

- **Active users** = 100,000,000

- **Orders/user/day** = 0.2
  ‚Üí **Daily orders** = 100,000,000 √ó 0.2 = **20,000,000 orders/day**

- Average seconds per day = 86,400
  ‚Üí **Avg orders/s** = 20,000,000 / 86,400 ‚âà **231.48 orders/s**

- Assume peak ‚âà 10√ó average (morning/lunch/dinner spikes)
  ‚Üí **Peak orders/s** ‚âà **2,315 orders/s**

- Average order event size ‚âà 2 KB (order created)
  ‚Üí **Peak ingress bandwidth for orders** ‚âà 2,315 √ó 2 KB/s ‚âà **4.63 MB/s** (~37 Mbps) ‚Äî small, but many supporting flows (images, maps, telemetry) add up.

### 2) Users & Sessions

- Peak concurrent users = 2% √ó 100M = **2,000,000 concurrent**
- If each active session keeps a websocket for real-time updates (order/status/location), this requires horizontal socket management (e.g., AWS API Gateway WebSockets / self-hosted socket clusters + autoscaling).

### 3) Restaurants & Media

- **Restaurants** = 2,000,000

- **Photos per restaurant** = 20

- **Avg photo size** = 0.5 MB
  ‚Üí **Total photos** = 2,000,000 √ó 20 = 40,000,000 photos
  ‚Üí **Storage** = 40,000,000 √ó 0.5 MB = 20,000,000 MB = **~20,000 GB = 20 TB**

- Add metadata, menus, icons ‚Äî small relative to photos (couple TB).

### 4) Search & Queries

- **Search queries/user/day** = 5
  ‚Üí **Total search queries/day** = 100,000,000 √ó 5 = **500,000,000/day**
  ‚Üí Avg search QPS = 500M / 86,400 ‚âà **5,787 qps**; peak maybe 3√ó ‚Üí **~17.4k qps**.
  Requires scaled Elasticsearch clusters with caching (Redis) and proper sharding.

### 5) Telemetry (Delivery Partner Location)

- Active delivery partners (concurrent reporting) say 10% of drivers report location frequently: 4M √ó 10% = 400k active reporters; each reports every 10s
  ‚Üí updates/s = 400,000 / 10 = **40,000 location updates/s**
  This is high-volume, low-latency stream -> Kafka + geo-partitioning + in-memory storage for live tracking.

### 6) Storage for Orders & Logs

- Avg order DB row size (including indexes) ‚âà 2 KB
  ‚Üí **Daily orders DB write** = 20M √ó 2 KB = 40,000,000 KB = ~40 GB/day
  ‚Üí Yearly = 40 GB √ó 365 ‚âà **14.6 TB/year** (raw; backups and indexes increase needs).

---

# üóÉÔ∏è Data Storage Strategy

|           Component |             Storage Type |                Example Technology |
| ------------------: | -----------------------: | --------------------------------: |
|       Orders (ACID) |  Relational DB (sharded) | PostgreSQL / Aurora / CockroachDB |
|       User Profiles |         Relational/NoSQL |                PostgreSQL + Redis |
| Restaurant Metadata |           Document store |                MongoDB / DynamoDB |
|      Menus & Config |               NoSQL / DB |              DynamoDB / Cassandra |
|      Media (photos) |     Object Storage + CDN |            Amazon S3 + CloudFront |
|  Real-time location | In-memory store + stream |               Redis (geo) + Kafka |
|        Search Index |            Search engine |        Elasticsearch / OpenSearch |
|           Analytics |    Data lake + warehouse |          S3 / BigQuery / Redshift |

---

# üö¶ Order Flow (Sequence - simplified)

sequenceDiagram
Client->>API Gateway: Place order request
API Gateway->>Auth Service: Validate token
API Gateway->>Order Service: Create order (transaction)
Order Service->>Payment Service: Charge customer
Payment Service->>Payment Gateway: Process payment
Order Service->>Restaurant Service: Send order to restaurant
Order Service->>Kafka: Emit order.created event
Delivery Service->>Dispatcher: Find & assign driver
Driver App->>Delivery Service: Accept order
Driver App->>Kafka: Send location updates
Notification Service->>Client: Push order & delivery updates

---

# üõ∞Ô∏è Delivery Optimization & Routing

- **Driver assignment**: capacity matching (ETA, proximity, acceptance rate), surge pricing incentives.
- **Routing**: map tile API + route optimization for multi-stop deliveries (when batching).
- **ETAs**: computed using live traffic + historical trip times (ML).
- **Batching**: combine nearby orders where feasible to reduce cost.
- **Dispatch latency target**: <2s for driver assignment.

---

# üß† Recommendation & Personalization

- **Offline batch models**: train on historical orders, time-of-day, cuisine affinity.
- **Real-time scoring**: feature store + online model (e.g., LightGBM / TensorFlow Serving).
- **Features**: recency, frequency, ratings, order context (meal time), location, price sensitivity, promotions.
- **Cold start**: use local popularity + demographic features.
- **A/B testing**: real-time experiments logged to analytics pipeline.

---

# üõ†Ô∏è Caching Strategy

|                               Item |              Tool |                 TTL/Notes |
| ---------------------------------: | ----------------: | ------------------------: |
|          Restaurant metadata (hot) |             Redis |                 5‚Äì30 mins |
|                         Menu items |             Redis | 10‚Äì60 mins (menus change) |
|           Search results (popular) |             Redis |                 30‚Äì60 sec |
|               User sessions/tokens |   Redis/Memcached |                    30 min |
| Live delivery state (user's order) | In-memory (Redis) |                 Real-time |

Benefits: reduce DB load, speed up frequently accessed flows (menus, top lists).

---

# üîÄ Eventing & Streaming

- **Apache Kafka** for event bus: order events, deliveries, payments, analytics events.
- **Topic partitioning** by region/restaurant to scale.
- **Consumers**: notification service, billing, analytics, recommendation real-time features.

---

# üîê Security & Compliance

- OAuth2 / JWT for auth, refresh tokens, device registration
- TLS everywhere
- PCI DSS compliance for card storage (use tokenization / PSP)
- GDPR/CCPA: data retention controls, user deletion
- Rate limiting and bot detection (search abuse)
- Fraud detection on payments (ML models + rules)

---

# üìà Scalability & Availability

- Microservices on Kubernetes (EKS/GKE/AKS) with HPA (CPU/RPS-based)
- Regional deployments with geo-routing & active-active failover
- Sharded databases (user ID / region) and read replicas
- CDN for static content (images, assets)
- Circuit breakers for external dependencies (payment gateways)
- Multi-AZ / multi-region for critical services (orders & payments)

---

# üîé Monitoring & Observability

|                    Metric |                        Toolset |
| ------------------------: | -----------------------------: |
| Orders/sec, failed orders |           Prometheus + Grafana |
|      Latency (API/Search) |                  ELK / Datadog |
|  Delivery partner metrics |    Custom dashboards + Grafana |
|        Business analytics | Kafka ‚Üí S3 ‚Üí Redshift/BigQuery |
|                   Tracing |         OpenTelemetry / Jaeger |

Set SLOs (e.g., 99.9% order placement success, 95% search latency <200ms).

---

# üßæ Example Cost/Capacity Notes (rough)

- **S3 for images**: 20 TB storage (from earlier) ‚Äî modest cost compared to traffic.
- **DB storage**: Orders / indices ‚Üí tens of TB/year as orders accumulate.
- **Kafka**: Provisioned for peak ingestion of location updates + order events (tens to hundreds of MB/s depending on retention).
- **CDN traffic**: heavy for images and map tiles; compute separately from dynamic API costs.

---

# ‚öñÔ∏è Trade-offs & Challenges

|                                Challenge | Mitigation                                                                |
| ---------------------------------------: | ------------------------------------------------------------------------- |
|                 Real-time tracking scale | Geo-partitioned Kafka + in-memory state, limit update frequency on mobile |
|                  High search/queries QPS | Sharding + cache + query coalescing                                       |
| Order consistency (payments + inventory) | Two-phase commit like flow, idempotency, compensating transactions        |
|           Driver supply/demand balancing | Dynamic incentives, forecasting models                                    |
|                      Fraud & chargebacks | Real-time fraud detection + manual review pipelines                       |

---

# ‚úÖ Tech Stack Summary (example)

- **Frontend**: React Native, Swift, Kotlin, React Web
- **API**: Node.js / Go / Java (Spring) microservices
- **API Gateway**: Kong / AWS API Gateway / Nginx
- **DB**: PostgreSQL (orders, sharded) / DynamoDB (menus) / Redis (cache)
- **Search**: Elasticsearch / OpenSearch
- **Streaming**: Kafka
- **Object Storage / CDN**: S3 + CloudFront (or GCS + Cloud CDN)
- **Orchestration**: Kubernetes + Helm
- **Payments**: Stripe/Adyen/Local PSPs
- **Maps**: Google Maps / Mapbox
- **Monitoring**: Prometheus / Grafana / ELK

---

# üìù Final Notes & Next Steps

- These numbers are **estimates** based on assumptions listed at top. If you want:

  - a smaller regional spec (e.g., 5M users) or a global scaling plan,
  - an entity-relationship diagram for DB schema (orders, menu, restaurants, drivers),
  - sequence diagrams for failure scenarios (payment failures, restaurant rejects order),
  - or a capacity plan with instance counts & cost estimate ‚Äî I can produce those next.
